{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845621cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "{'everyone', '‘d', 'any', 'top', 'whereafter', 'though', 'really', 'except', 'two', 'too', 'someone', 'hereby', \"'ve\", 'n‘t', 'across', 'whereupon', 'seem', 'all', 'most', 'him', 'them', 'becoming', '’re', 'used', 'ca', 'much', 'more', 'because', 'quite', 'can', 'what', 'throughout', 'few', \"'d\", 'whence', 'me', 'somewhere', 'therein', 'either', 'whose', 'below', 'yourself', 'thence', 'during', 'whenever', 'around', 'meanwhile', 'very', 'for', 'also', 'onto', 'yet', 'twelve', 'enough', 'or', 'themselves', 'now', 'but', 'down', 'side', 'four', 'does', 'under', 'forty', 'something', 'next', 'hence', 'and', 'some', 'no', 'mine', 'anything', 'has', 'call', 'regarding', 'thereupon', 'as', 'these', 'see', 'herein', 'to', 'often', 'out', 'seeming', 'name', 'was', 'indeed', '‘re', 'every', 'off', 'thereby', 'whether', 'whither', 'n’t', 'therefore', 'doing', 'wherever', 're', 'only', 'we', \"n't\", 'get', 'further', 'sometime', 'would', 'been', 'seems', 'whoever', 'my', 'thus', 'least', 'she', 'however', 'whereby', 'each', 'such', '‘s', 'unless', 'do', 'last', 'various', 'might', 'so', 'beforehand', 'against', 'together', 'whatever', '‘m', 'move', 'on', 'otherwise', 'itself', 'using', 'say', 'before', 'ours', 'of', 'per', 'done', 'several', 'upon', 'which', 'already', 'back', '’ve', 'should', 'own', 'not', 'above', 'afterwards', 'did', 'same', 'thereafter', 'into', '‘ll', 'himself', 'former', 'our', '’ll', 'had', 'full', 'becomes', 'wherein', 'please', 'moreover', 'nine', 'almost', 'others', 'toward', 'amount', 'well', 'first', 'front', \"'re\", 'eleven', 'within', 'an', 'then', 'other', 'although', 'one', \"'m\", 'never', 'could', 'is', 'neither', '‘ve', 'by', 'i', 'third', 'among', 'everything', 'become', 'latterly', 'between', 'may', 'why', '’d', 'nobody', 'take', '’s', 'go', 'here', 'noone', 'bottom', 'herself', 'it', 'make', 'nevertheless', 'ourselves', 'without', 'serious', 'always', 'who', \"'ll\", 'hers', 'another', 'its', 'sometimes', 'must', 'hereupon', 'through', 'less', 'ten', 'that', 'there', 'both', 'while', 'from', 'once', 'ever', 'his', 'else', 'your', 'empty', 'whole', 'put', 'fifty', 'their', 'show', 'five', 'anyhow', 'anywhere', 'you', 'am', 'still', 'towards', 'alone', 'her', 'again', 'whom', 'over', 'even', 'myself', 'anyone', 'beside', 'elsewhere', 'twenty', \"'s\", 'sixty', 'three', 'made', 'perhaps', 'if', 'the', 'via', 'due', 'part', 'after', 'behind', 'became', 'with', 'those', 'how', 'in', 'latter', 'besides', 'they', 'he', 'when', 'thru', 'nor', 'hundred', 'anyway', 'were', 'yourselves', 'everywhere', 'along', 'just', 'mostly', 'will', 'none', 'fifteen', 'give', 'rather', 'beyond', 'up', 'being', 'amongst', 'many', 'eight', 'formerly', 'nothing', 'keep', 'somehow', 'until', 'a', 'seemed', '’m', 'cannot', 'this', 'where', 'six', 'at', 'than', 'hereafter', 'about', 'be', 'namely', 'us', 'have', 'since', 'nowhere', 'are', 'whereas', 'yours'}\n"
     ]
    }
   ],
   "source": [
    "#TASK - 1\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English                             \n",
    "en = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = en.Defaults.stop_words\n",
    "\n",
    "print(len(stopwords))\n",
    "print(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c0fdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First fifteen stop words: ['behind', 'along', 'after', 'so', 'there', 'been', 'name', 'therein', 'others', 'nobody', 'another', 'at', 'such', 'her', 'done']\n"
     ]
    }
   ],
   "source": [
    "#TASK - 1 contd.\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS                     # Importing stop words from English language.\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))                  # Printing the total number of stop words\n",
    "print('First fifteen stop words: %s' % list(spacy_stopwords)[:15])            # Printing first few stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d628ce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stemming: cries --> cri\n",
      "After stemming: this --> this\n",
      "After stemming: lied --> lie\n",
      "After stemming: computing --> comput\n",
      "After stemming: organizing --> organ\n",
      "After stemming: matches --> match\n",
      "\n",
      "\n",
      "After lemmatization: cries cry\n",
      "After lemmatization: this this\n",
      "After lemmatization: lied lie\n",
      "After lemmatization: computing computing\n",
      "After lemmatization: organizing organizing\n",
      "After lemmatization: matches match\n"
     ]
    }
   ],
   "source": [
    "#TASK - 2\n",
    "\n",
    "import spacy \n",
    "import nltk                                                                                 # import nltk library natural language toolkit library\n",
    "from nltk.stem.snowball import SnowballStemmer                                               # import stemmer\n",
    "nlp = spacy.load(\"en_core_web_sm\")                                                           # Load model\n",
    "stemmer = SnowballStemmer(language='english')                                                # import stemmer\n",
    "tokens = ['cries', 'this', 'lied', 'computing', 'organizing', 'matches']                     # Input words\n",
    "for token in tokens:                                                                         # For all words\n",
    "    print('After stemming:',token + ' --> ' + stemmer.stem(token))                           # Print result after stemming\n",
    "doc = nlp(\"cries this lied computing organizing matches\")                                    # For lemmatization\n",
    "print('\\n')                                                                                  # print empty line for differentiating\n",
    "for word in doc:                                                                             # For all words\n",
    "    print('After lemmatization:',word.text,word.lemma_)                                      # Print result after lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7a5dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note from poster to Kubrick newsgroup:\n",
      "\n",
      "I found this on a bbs a while ago and I thought I'd pass it along to all \n",
      "of you Kubrick freaks out there.\n",
      "\n",
      "02/23/89\n",
      "Transcriber's note:\n",
      "\n",
      "For all you Clarke/Kubrick/2001 fans,\n",
      "\n",
      "I found the original paper copy of this screenplay a while back and felt \n",
      "compelled to transcribe it to disk and upload it to various bulletin \n",
      "boards for the enjoyment of all.\n",
      "\n",
      "The final movie deviates from this screenplay in a number of interesting \n",
      "ways. I've tried to maintain the format of the original document except \n",
      "the number of lines per page of the original. In order to reduce the \n",
      "length of this file I've used a bar of \"------\" to delimit the pages as \n",
      "there was a lot of whitespace per original screenplay page.\n"
     ]
    }
   ],
   "source": [
    "#TASK - 3\n",
    "\n",
    "import spacy                                                  # Import spaCy library\n",
    "from spacy.lang.en import English                             # Import specific model\n",
    "nlp = spacy.load(\"en_core_web_sm\")                            # Load model\n",
    "f = open(\"scifiscripts_intro.txt\")\n",
    "contents = f.read()                                           # Read input text\n",
    "print(contents)  # To print all contents of file              # print contents\n",
    "text = str(contents)                                          # convert to string type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82833f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text :  Note from poster to Kubrick newsgroup:\n",
      "\n",
      "I found this on a bbs a while ago and I thought I'd pass it along to all \n",
      "of you Kubrick freaks out there.\n",
      "\n",
      "02/23/89\n",
      "Transcriber's note:\n",
      "\n",
      "For all you Clarke/Kubrick/2001 fans,\n",
      "\n",
      "I found the original paper copy of this screenplay a while back and felt \n",
      "compelled to transcribe it to disk and upload it to various bulletin \n",
      "boards for the enjoyment of all.\n",
      "\n",
      "The final movie deviates from this screenplay in a number of interesting \n",
      "ways. I've tried to maintain the format of the original document except \n",
      "the number of lines per page of the original. In order to reduce the \n",
      "length of this file I've used a bar of \"------\" to delimit the pages as \n",
      "there was a lot of whitespace per original screenplay page.\n",
      "Note from poster to Kubrick newsgroup:\n",
      "\n",
      "I found this on a bbs a while ago and I thought I'd pass it along to all \n",
      "of you Kubrick freaks out there.\n",
      "\n",
      "02/23/89\n",
      "Transcriber's note:\n",
      "\n",
      "For all you Clarke/Kubrick/2001 fans,\n",
      "\n",
      "I found the original paper copy of this screenplay a while back and felt \n",
      "compelled to transcribe it to disk and upload it to various bulletin \n",
      "boards for the enjoyment of all.\n",
      "\n",
      "The final movie deviates from this screenplay in a number of interesting \n",
      "ways. I've tried to maintain the format of the original document except \n",
      "the number of lines per page of the original. In order to reduce the \n",
      "length of this file I've used a bar of \"------\" to delimit the pages as \n",
      "there was a lot of whitespace per original screenplay page. \n",
      "\n",
      "\n",
      "Text after removing stopwords\n",
      "NoteposterKubricknewsgroup:foundbbsagothoughtI'dpassKubrickfreaksthere.02/23/89Transcriber'snote:Clarke/Kubrick/2001fans,foundoriginalpapercopyscreenplayfeltcompelledtranscribediskuploadbulletinboardsenjoymentall.finalmoviedeviatesscreenplaynumberinterestingways.I'vetriedmaintainformatoriginaldocumentnumberlinespageoriginal.orderreducelengthfileI'vebar\"------\"delimitpageslotwhitespaceoriginalscreenplaypage.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "with open(\"scifiscripts_intro.txt\") as f:\n",
    "    text=f.read()\n",
    "lst = []\n",
    "for token in text.split():\n",
    "    if token.lower() not in stopwords:        \n",
    "        lst.append(token)\n",
    "    \n",
    "print(\"Original text : \",text)\n",
    "print(text, '\\n\\n')\n",
    "print(\"Text after removing stopwords\")\n",
    "print(''.join(lst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab186376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found this on a bbs a while ago and I thought I'd pass it along to all \n",
      "of you Kubrick freaks out there.\n",
      "I found the original paper copy of this screenplay a while back and felt \n",
      "compelled to transcribe it to disk and upload it to various bulletin \n",
      "boards for the enjoyment of all.\n",
      "\n",
      "The final movie deviates from this screenplay in a number of interesting \n",
      "ways. I've tried to maintain the format of the original document except \n",
      "the number of lines per page of the original. In order to reduce the \n",
      "length of this file I've used a bar of \"------\" to delimit the pages as \n",
      "there was a lot of whitespace per original screenplay page. \n",
      "\n",
      "\n",
      "Text after removing stopwords\n",
      "foundbbsagothoughtI'dpassKubrickfreaksthere.foundoriginalpapercopyscreenplayfeltcompelledtranscribediskuploadbulletinboardsenjoymentall.finalmoviedeviatesscreenplaynumberinterestingways.I'vetriedmaintainformatoriginaldocumentnumberlinespageoriginal.orderreducelengthfileI'vebar\"------\"delimitpageslotwhitespaceoriginalscreenplaypage.\n"
     ]
    }
   ],
   "source": [
    "#B. FOR FIRST FIVE SENTENCES\n",
    "\n",
    "import spacy                                                  # Import spaCy library\n",
    "from spacy.lang.en import English                             # Import specific model\n",
    "nlp = spacy.load(\"en_core_web_sm\")                            # Load model\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "with open('First five sentences.txt') as f:\n",
    "        text=f.read()\n",
    "lst = []\n",
    "for token in text.split():\n",
    "    if token.lower() not in stopwords:        \n",
    "        lst.append(token)\n",
    "    \n",
    "#print(\"Original text : \",text)\n",
    "print(text, '\\n\\n')\n",
    "print(\"Text after removing stopwords\")\n",
    "print(''.join(lst))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51bab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bryan \t PROPN \t NNP\n",
      "visited \t VERB \t VBD\n",
      "his \t PRON \t PRP$\n",
      "friend \t NOUN \t NN\n",
      "for \t ADP \t IN\n",
      "a \t DET \t DT\n",
      "while \t NOUN \t NN\n",
      "and \t CCONJ \t CC\n",
      "then \t ADV \t RB\n",
      "went \t VERB \t VBD\n",
      "home \t ADV \t RB\n",
      "at \t ADP \t IN\n",
      "10 \t NUM \t CD\n",
      "pm \t NOUN \t NN\n",
      ". \t PUNCT \t .\n"
     ]
    }
   ],
   "source": [
    "#TASK - 4\n",
    "#Bryan visited his friend for a while and then went home at 10 pm.\n",
    "\n",
    "doc = nlp(\"Bryan visited his friend for a while and then went home at 10 pm.\")                # Input sentence\n",
    "for word in doc:                                                                              # For all input words\n",
    "    print(word.text,'\\t',word.pos_,'\\t',word.tag_)                                            # Apply POS and print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ead285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA \t PROPN \t NNP\n",
      "HIGH \t PROPN \t NNP\n",
      "SCHOOL \t PROPN \t NNP\n",
      "- \t PUNCT \t HYPH\n",
      "DAY \t PROPN \t NNP\n",
      "\n",
      " \t SPACE \t _SP\n",
      "Revision \t PROPN \t NNP\n",
      "November \t PROPN \t NNP\n",
      "12 \t NUM \t CD\n",
      ", \t PUNCT \t ,\n",
      "1997 \t NUM \t CD\n",
      "\n",
      " \t SPACE \t _SP\n"
     ]
    }
   ],
   "source": [
    "#TASK - 5\n",
    "\n",
    "import spacy\n",
    "f = open('Random.txt')\n",
    "contents = f.read()                                                             # Read input data\n",
    "text = str(contents)                                                            # string type\n",
    "doc = nlp(text)                                                                 # convert to NLP object\n",
    "for word in doc:                                                                # For all input words\n",
    "    print(word.text,'\\t',word.pos_,'\\t',word.tag_)                              # Apply POS and print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e74eefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'everyone', '‘d', 'any', 'top', 'whereafter', 'though', 'really', 'except', 'Arfat', 'two', 'too', 'someone', 'hereby', \"'ve\", 'n‘t', 'across', 'whereupon', 'seem', 'all', 'most', 'him', 'them', 'becoming', '’re', 'used', 'ca', 'much', 'more', 'because', 'quite', 'can', 'what', 'throughout', 'few', \"'d\", 'whence', 'me', 'somewhere', 'therein', 'either', 'whose', 'below', 'yourself', 'thence', 'during', 'whenever', 'around', 'meanwhile', 'very', 'for', 'also', 'onto', 'yet', 'twelve', 'enough', 'or', 'themselves', 'now', 'but', 'down', 'side', 'four', 'does', 'under', 'forty', 'something', 'next', 'hence', 'and', 'some', 'no', 'mine', 'anything', 'has', 'call', 'KUKU', 'regarding', 'thereupon', 'as', 'these', 'see', 'herein', 'to', 'often', 'out', 'seeming', 'name', 'was', 'indeed', '‘re', 'every', 'off', 'thereby', 'whether', 'whither', 'n’t', 'therefore', 'doing', 'wherever', 're', 'only', 'we', \"n't\", 'get', 'further', 'sometime', 'would', 'been', 'seems', 'whoever', 'my', 'thus', 'least', 'she', 'however', 'whereby', 'Prem', 'each', 'such', '‘s', 'unless', 'do', 'last', 'various', 'might', 'so', 'beforehand', 'against', 'together', 'whatever', '‘m', 'move', 'on', 'otherwise', 'itself', 'using', 'say', 'before', 'ours', 'of', 'per', 'done', 'several', 'upon', 'which', 'already', 'back', '’ve', 'should', 'own', 'not', 'above', 'afterwards', 'did', 'same', 'thereafter', 'into', '‘ll', 'himself', 'former', 'our', '’ll', 'had', 'full', 'becomes', 'wherein', 'please', 'moreover', 'nine', 'almost', 'others', 'toward', 'amount', 'well', 'first', 'front', \"'re\", 'eleven', 'within', 'an', 'then', 'other', 'although', 'one', \"'m\", 'never', 'could', 'is', 'neither', '‘ve', 'by', 'i', 'third', 'among', 'everything', 'become', 'latterly', 'between', 'may', 'why', '’d', 'nobody', 'take', '’s', 'go', 'here', 'noone', 'bottom', 'herself', 'it', 'Rashmi', 'make', 'nevertheless', 'ourselves', 'without', 'serious', 'always', 'who', \"'ll\", 'hers', 'another', 'its', 'sometimes', 'must', 'hereupon', 'through', 'less', 'ten', 'that', 'there', 'both', 'while', 'from', 'once', 'ever', 'his', 'else', 'your', 'empty', 'whole', 'put', 'fifty', 'their', 'show', 'five', 'anyhow', 'anywhere', 'you', 'am', 'still', 'towards', 'alone', 'her', 'again', 'whom', 'over', 'even', 'myself', 'anyone', 'beside', 'elsewhere', 'twenty', \"'s\", 'sixty', 'Manish', 'three', 'made', 'perhaps', 'if', 'the', 'via', 'due', 'part', 'after', 'behind', 'became', 'with', 'those', 'how', 'in', 'latter', 'besides', 'they', 'he', 'when', 'thru', 'nor', 'hundred', 'anyway', 'were', 'yourselves', 'everywhere', 'along', 'just', 'mostly', 'will', 'none', 'fifteen', 'give', 'rather', 'beyond', 'up', 'being', 'amongst', 'many', 'eight', 'formerly', 'nothing', 'keep', 'somehow', 'until', 'a', 'seemed', '’m', 'cannot', 'this', 'where', 'six', 'at', 'than', 'hereafter', 'about', 'be', 'namely', 'us', 'have', 'since', 'nowhere', 'are', 'whereas', 'yours'}\n"
     ]
    }
   ],
   "source": [
    "#TASK - 6\n",
    "\n",
    "#Adding stopwords\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.Defaults.stop_words |= {\"Rashmi\", \"Prem\", \"Arfat\", \"Manish\", \"KUKU\"}\n",
    "print(nlp.Defaults.stop_words)                                              # To print the updated set of stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1928650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'everyone', '‘d', 'any', 'top', 'whereafter', 'though', 'really', 'except', 'Arfat', 'two', 'too', 'someone', 'hereby', \"'ve\", 'n‘t', 'across', 'whereupon', 'seem', 'all', 'most', 'him', 'them', 'becoming', '’re', 'used', 'ca', 'much', 'more', 'because', 'quite', 'can', 'what', 'throughout', 'few', \"'d\", 'whence', 'me', 'somewhere', 'therein', 'either', 'whose', 'below', 'yourself', 'thence', 'during', 'whenever', 'around', 'meanwhile', 'very', 'for', 'also', 'onto', 'yet', 'twelve', 'enough', 'or', 'themselves', 'now', 'but', 'down', 'side', 'four', 'does', 'under', 'forty', 'something', 'next', 'hence', 'and', 'some', 'no', 'mine', 'anything', 'has', 'call', 'KUKU', 'regarding', 'thereupon', 'as', 'these', 'see', 'herein', 'to', 'often', 'out', 'seeming', 'name', 'was', 'indeed', '‘re', 'every', 'off', 'thereby', 'whether', 'whither', 'n’t', 'therefore', 'doing', 'wherever', 're', 'only', 'we', \"n't\", 'get', 'further', 'sometime', 'would', 'been', 'seems', 'whoever', 'my', 'thus', 'least', 'she', 'however', 'whereby', 'Prem', 'each', 'such', '‘s', 'unless', 'do', 'last', 'various', 'might', 'so', 'beforehand', 'against', 'together', 'whatever', '‘m', 'move', 'on', 'otherwise', 'itself', 'using', 'say', 'before', 'ours', 'of', 'per', 'done', 'several', 'upon', 'which', 'already', 'back', '’ve', 'should', 'own', 'not', 'above', 'afterwards', 'did', 'same', 'thereafter', 'into', '‘ll', 'himself', 'former', 'our', '’ll', 'had', 'full', 'wherein', 'please', 'moreover', 'nine', 'almost', 'others', 'toward', 'amount', 'well', 'first', 'front', \"'re\", 'eleven', 'within', 'an', 'then', 'other', 'although', 'one', \"'m\", 'could', 'is', 'neither', '‘ve', 'by', 'i', 'third', 'among', 'everything', 'become', 'latterly', 'may', 'why', '’d', 'nobody', 'take', '’s', 'go', 'here', 'noone', 'bottom', 'herself', 'it', 'Rashmi', 'make', 'nevertheless', 'ourselves', 'without', 'serious', 'who', \"'ll\", 'hers', 'another', 'its', 'sometimes', 'must', 'hereupon', 'through', 'less', 'ten', 'that', 'there', 'both', 'while', 'from', 'once', 'ever', 'his', 'else', 'your', 'empty', 'whole', 'put', 'fifty', 'their', 'show', 'five', 'anyhow', 'anywhere', 'you', 'am', 'still', 'towards', 'alone', 'her', 'again', 'whom', 'over', 'even', 'myself', 'anyone', 'beside', 'elsewhere', 'twenty', \"'s\", 'sixty', 'Manish', 'three', 'made', 'perhaps', 'if', 'the', 'via', 'due', 'part', 'after', 'behind', 'became', 'with', 'those', 'how', 'in', 'latter', 'besides', 'they', 'he', 'when', 'thru', 'nor', 'hundred', 'anyway', 'were', 'yourselves', 'everywhere', 'along', 'just', 'mostly', 'will', 'none', 'fifteen', 'give', 'rather', 'beyond', 'up', 'being', 'amongst', 'many', 'eight', 'formerly', 'nothing', 'keep', 'somehow', 'until', 'a', 'seemed', '’m', 'cannot', 'this', 'where', 'six', 'at', 'than', 'hereafter', 'about', 'be', 'namely', 'us', 'have', 'since', 'nowhere', 'are', 'whereas', 'yours'}\n"
     ]
    }
   ],
   "source": [
    "#Removing stopwords\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.Defaults.stop_words -={\"always\", \"never\", \"between\", \"becomes\"}\n",
    "print(nlp.Defaults.stop_words)                                              # To print the updated set of stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1e86a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA HIGH SCHOOL - DAY\n",
      "Revision November 12, 1997\n",
      "I hope dinner's ready because I only have ten minutes before Mrs. Johnson squirts out a screamer.\n",
      "He grabs the mail and rifles through it, as he bends down to kiss Sharon on the cheek.\n",
      "MICHAEL- C'mon. I'm supposed to give you the tour. They head out of the office\n",
      "MICHAEL (continuing)- So -- which Dakota you from?\n",
      "          \n",
      "                                 \n",
      "\n",
      "PADUA\n",
      "HIGH\n",
      "SCHOOL\n",
      "-\n",
      "DAY\n",
      "\n",
      "\n",
      "Revision\n",
      "November\n",
      "12\n",
      ",\n",
      "1997\n",
      "\n",
      "\n",
      "I\n",
      "hope\n",
      "dinner\n",
      "'s\n",
      "ready\n",
      "because\n",
      "I\n",
      "only\n",
      "have\n",
      "ten\n",
      "minutes\n",
      "before\n",
      "Mrs.\n",
      "Johnson\n",
      "squirts\n",
      "out\n",
      "a\n",
      "screamer\n",
      ".\n",
      "\n",
      "\n",
      "He\n",
      "grabs\n",
      "the\n",
      "mail\n",
      "and\n",
      "rifles\n",
      "through\n",
      "it\n",
      ",\n",
      "as\n",
      "he\n",
      "bends\n",
      "down\n",
      "to\n",
      "kiss\n",
      "Sharon\n",
      "on\n",
      "the\n",
      "cheek\n",
      ".\n",
      "\n",
      "\n",
      "MICHAEL-\n",
      "C'm\n",
      "on\n",
      ".\n",
      "I\n",
      "'m\n",
      "supposed\n",
      "to\n",
      "give\n",
      "you\n",
      "the\n",
      "tour\n",
      ".\n",
      "They\n",
      "head\n",
      "out\n",
      "of\n",
      "the\n",
      "office\n",
      "\n",
      "\n",
      "MICHAEL\n",
      "(\n",
      "continuing)-\n",
      "So\n",
      "--\n",
      "which\n",
      "Dakota\n",
      "you\n",
      "from\n",
      "?\n",
      "\n",
      "          \n",
      "                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TASK - 7\n",
    "\n",
    "#TOKENIZATION\n",
    "\n",
    "from spacy.lang.en import English                              # Importing library\n",
    "nlp = English()                                                # Importing model\n",
    "nlp = spacy.load(\"en_core_web_sm\")                             # Importing model\n",
    "f = open(\"Raw_data_for_analysis.txt\")                              #No need to specify the path speciy file name instead of path\n",
    "contents = f.read()                                            # To read input dataset\n",
    "contents = contents[:500]                                      # First few characters of input file\n",
    "print(contents)                                                # To print contents of file\n",
    "text_combined = str(contents)                                  # String\n",
    "doc = nlp(text_combined)                                       # Create NLP object\n",
    "for token in doc:\n",
    "    print(token)                                               # Print tokens\n",
    "len(token)                                                    # Length of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ab0d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 327\n",
      "First ten stop words: ['everyone', '‘d', 'any', 'top', 'whereafter', 'though', 'really', 'except', 'Arfat', 'two']\n",
      "\n",
      " \n",
      "Filtered Sentence: [PADUA, HIGH, SCHOOL, -, DAY, \n",
      ", Revision, November, 12, ,, 1997, \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS                     # Importing stop words from English language.\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))                  # Printing the total number of stop words\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])            # Printing first few stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS                           # Importing stop words\n",
    "filtered_sent=[]                                                          # Initialize empty file\n",
    "doc = nlp(text)                                                           # \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "for word in doc:                                                          # For all words\n",
    "    if word.is_stop==False:                                               # check condition\n",
    "        filtered_sent.append(word)                                        # Appending words in output file\n",
    "print(\"\\n \\nFiltered Sentence:\",filtered_sent)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43184fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text :  PADUA HIGH SCHOOL - DAY\n",
      "Revision November 12, 1997\n",
      "I hope dinner's ready because I only have ten minutes before Mrs. Johnson squirts out a screamer.\n",
      "He grabs the mail and rifles through it, as he bends down to kiss Sharon on the cheek.\n",
      "MICHAEL- C'mon. I'm supposed to give you the tour. They head out of the office\n",
      "MICHAEL (continuing)- So -- which Dakota you from?\n",
      "          \n",
      "                                 \n",
      "\n",
      "PADUA HIGH SCHOOL - DAY\n",
      "Revision November 12, 1997\n",
      "I hope dinner's ready because I only have ten minutes before Mrs. Johnson squirts out a screamer.\n",
      "He grabs the mail and rifles through it, as he bends down to kiss Sharon on the cheek.\n",
      "MICHAEL- C'mon. I'm supposed to give you the tour. They head out of the office\n",
      "MICHAEL (continuing)- So -- which Dakota you from?\n",
      "          \n",
      "                                 \n",
      " \n",
      "\n",
      "\n",
      "Text after removing stopwords\n",
      "PADUAHIGHSCHOOL-DAYRevisionNovember12,1997hopedinner'sreadyminutesMrs.Johnsonsquirtsscreamer.grabsmailriflesit,bendskissSharoncheek.MICHAEL-C'mon.I'msupposedtour.headofficeMICHAEL(continuing)---Dakotafrom?\n"
     ]
    }
   ],
   "source": [
    "#REMOVING STOP WORDS\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "with open(\"Raw_data_for_analysis.txt\") as f:\n",
    "    text=f.read()\n",
    "lst = []\n",
    "for token in text.split():\n",
    "    if token.lower() not in stopwords:        \n",
    "        lst.append(token)\n",
    "    \n",
    "print(\"Original text : \",text)\n",
    "print(text, '\\n\\n')\n",
    "print(\"Text after removing stopwords\")\n",
    "print(''.join(lst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a62906e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ->  I\n",
      "hope ->  hope\n",
      "dinner ->  dinner\n",
      "'s ->  's\n",
      "ready ->  ready\n",
      "because ->  because\n",
      "I ->  I\n",
      "only ->  only\n",
      "have ->  have\n",
      "ten ->  ten\n",
      "minutes ->  minute\n",
      "before ->  before\n",
      "Mrs. ->  Mrs.\n",
      "Johnson ->  Johnson\n",
      "squirts ->  squirt\n",
      "out ->  out\n",
      "a ->  a\n",
      "screamer ->  screamer\n",
      ". ->  .\n",
      "He ->  he\n",
      "grabs ->  grab\n",
      "the ->  the\n",
      "mail ->  mail\n",
      "and ->  and\n",
      "rifles ->  rifle\n",
      "through ->  through\n",
      "it ->  it\n",
      ", ->  ,\n",
      "as ->  as\n",
      "he ->  he\n",
      "bends ->  bend\n",
      "down ->  down\n",
      "to ->  to\n",
      "kiss ->  kiss\n",
      "Sharon ->  Sharon\n",
      "on ->  on\n",
      "the ->  the\n",
      "cheek ->  cheek\n",
      ". ->  .\n",
      "MICHAEL- ->  michael-\n",
      "C'm ->  come\n",
      "on ->  on\n",
      ". ->  .\n",
      "I ->  I\n",
      "'m ->  be\n",
      "supposed ->  suppose\n",
      "to ->  to\n",
      "give ->  give\n",
      "you ->  you\n",
      "the ->  the\n",
      "tour ->  tour\n",
      ". ->  .\n",
      "They ->  they\n",
      "head ->  head\n",
      "out ->  out\n",
      "of ->  of\n",
      "the ->  the\n",
      "officeMICHAEL ->  officeMICHAEL\n",
      "( ->  (\n",
      "continuing)- ->  continuing)-\n",
      "So ->  so\n",
      "-- ->  --\n",
      "which ->  which\n",
      "Dakota ->  Dakota\n",
      "you ->  you\n",
      "from ->  from\n",
      "? ->  ?\n"
     ]
    }
   ],
   "source": [
    "#LEMMITZATION\n",
    "\n",
    "doc = nlp(\"I hope dinner's ready because I only have ten minutes before Mrs. Johnson squirts out a screamer.He grabs the mail and rifles through it, as he bends down to kiss Sharon on the cheek.MICHAEL- C'mon. I'm supposed to give you the tour. They head out of the officeMICHAEL (continuing)- So -- which Dakota you from?\")          # Input\n",
    "for word in doc:                                                      # For all words\n",
    "    print(word.text,'-> ',word.lemma_)               # Print result after lemmatization\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43f6c8b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA PADUA\n",
      "HIGH HIGH\n",
      "SCHOOL SCHOOL\n",
      "- -\n",
      "DAY DAY\n",
      "\n",
      " \n",
      "\n",
      "Revision Revision\n",
      "November November\n",
      "12 12\n",
      ", ,\n",
      "1997 1997\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in filtered_sent:                      # All words in input text                 \n",
    "    print(word.text,word.lemma_)                # After Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c27d3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA \t PROPN \t NNP\n",
      "HIGH \t PROPN \t NNP\n",
      "SCHOOL \t PROPN \t NNP\n",
      "- \t PUNCT \t HYPH\n",
      "DAY \t PROPN \t NNP\n",
      "\n",
      " \t SPACE \t _SP\n",
      "Revision \t PROPN \t NNP\n",
      "November \t PROPN \t NNP\n",
      "12 \t NUM \t CD\n",
      ", \t PUNCT \t ,\n",
      "1997 \t NUM \t CD\n",
      "\n",
      " \t SPACE \t _SP\n",
      "I \t PRON \t PRP\n",
      "hope \t VERB \t VBP\n",
      "dinner \t NOUN \t NN\n",
      "'s \t PART \t POS\n",
      "ready \t ADJ \t JJ\n",
      "because \t SCONJ \t IN\n",
      "I \t PRON \t PRP\n",
      "only \t ADV \t RB\n",
      "have \t VERB \t VBP\n",
      "ten \t NUM \t CD\n",
      "minutes \t NOUN \t NNS\n",
      "before \t SCONJ \t IN\n",
      "Mrs. \t PROPN \t NNP\n",
      "Johnson \t PROPN \t NNP\n",
      "squirts \t VERB \t VBZ\n",
      "out \t ADP \t RP\n",
      "a \t DET \t DT\n",
      "screamer \t NOUN \t NN\n",
      ". \t PUNCT \t .\n",
      "\n",
      " \t SPACE \t _SP\n",
      "He \t PRON \t PRP\n",
      "grabs \t VERB \t VBZ\n",
      "the \t DET \t DT\n",
      "mail \t NOUN \t NN\n",
      "and \t CCONJ \t CC\n",
      "rifles \t NOUN \t NNS\n",
      "through \t ADP \t IN\n",
      "it \t PRON \t PRP\n",
      ", \t PUNCT \t ,\n",
      "as \t SCONJ \t IN\n",
      "he \t PRON \t PRP\n",
      "bends \t VERB \t VBZ\n",
      "down \t ADP \t RP\n",
      "to \t PART \t TO\n",
      "kiss \t VERB \t VB\n",
      "Sharon \t PROPN \t NNP\n",
      "on \t ADP \t IN\n",
      "the \t DET \t DT\n",
      "cheek \t NOUN \t NN\n",
      ". \t PUNCT \t .\n",
      "\n",
      " \t SPACE \t _SP\n",
      "MICHAEL- \t PROPN \t NNP\n",
      "C'm \t VERB \t VBZ\n",
      "on \t ADP \t RP\n",
      ". \t PUNCT \t .\n",
      "I \t PRON \t PRP\n",
      "'m \t AUX \t VBP\n",
      "supposed \t VERB \t VBN\n",
      "to \t PART \t TO\n",
      "give \t VERB \t VB\n",
      "you \t PRON \t PRP\n",
      "the \t DET \t DT\n",
      "tour \t NOUN \t NN\n",
      ". \t PUNCT \t .\n",
      "They \t PRON \t PRP\n",
      "head \t VERB \t VBP\n",
      "out \t ADP \t IN\n",
      "of \t ADP \t IN\n",
      "the \t DET \t DT\n",
      "office \t NOUN \t NN\n",
      "\n",
      " \t SPACE \t _SP\n",
      "MICHAEL \t PROPN \t NNP\n",
      "( \t PUNCT \t -LRB-\n",
      "continuing)- \t NOUN \t NNS\n",
      "So \t ADV \t RB\n",
      "-- \t PUNCT \t :\n",
      "which \t PRON \t WDT\n",
      "Dakota \t PROPN \t NNP\n",
      "you \t PRON \t PRP\n",
      "from \t ADP \t IN\n",
      "? \t PUNCT \t .\n",
      "\n",
      "          \n",
      "                                 \n",
      " \t SPACE \t _SP\n"
     ]
    }
   ],
   "source": [
    "#POS TAGGING\n",
    "\n",
    "#import spacy\n",
    "#from spacy.lang.en import English\n",
    "doc = nlp(\"en_core_web_sm\")\n",
    "f = open(\"Raw_data_for_analysis.txt\")\n",
    "contents = f.read()                                                             # Read input data\n",
    "text = str(contents)                                                            # string type\n",
    "doc = nlp(text)                                                                 # convert to NLP object\n",
    "for word in doc:                                                                # For all input words\n",
    "    print(word.text,'\\t',word.pos_,'\\t',word.tag_)                             # Apply POS and print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8bf4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215377aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

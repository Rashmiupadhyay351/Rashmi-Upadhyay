{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1103e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy                                                  # Import spaCy library\n",
    "from spacy.lang.en import English                             # Import specific model\n",
    "nlp = spacy.load(\"en_core_web_sm\")                            # Load model\n",
    "import collections\n",
    "from typing import Dict, List, Tuple                          # import dictionaries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68416c3d",
   "metadata": {},
   "source": [
    "### TASK - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0470e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOW Representation: \n",
      " [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n",
      "Input Text:\n",
      " Review 1 : This movie is very scary and long\n",
      "\n",
      "Dictionary: \n",
      " {'Review': 0, '1': 1, ':': 2, 'This': 3, 'movie': 4, 'is': 5, 'very': 6, 'scary': 7, 'and': 8, 'long': 9}\n"
     ]
    }
   ],
   "source": [
    "#REVIEW 1\n",
    "\n",
    "def text2bow(words: List[str], dictionary: Dict[str, int]) -> List[Tuple[int, int]]:                  # Text to BOW\n",
    "    word_frequences = collections.defaultdict(int)\n",
    "    for word in words:\n",
    "        if word not in dictionary:                                                                    # Check condition\n",
    "            dictionary[word] = len(dictionary)\n",
    "        word_frequences[dictionary[word]] += 1\n",
    "    return list(word_frequences.items())                                                              # Return word frequencies\n",
    "sample_text = 'Review 1 : This movie is very scary and long'                                                     # Input text\n",
    "dictionary = {}                                                                                       # Initialize dictionary\n",
    "print('\\nBOW Representation: \\n', text2bow(sample_text.split(), dictionary))                          # print BOW\n",
    "print('Input Text:\\n',sample_text)                                                                    # print input\n",
    "print('\\nDictionary: \\n', dictionary)                                                                 # print dictionary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23de62ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOW Representation: \n",
      " [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1)]\n",
      "Input Text:\n",
      " Review 2 : This movie is not scary and is slow\n",
      "\n",
      "Dictionary: \n",
      " {'Review': 0, '2': 1, ':': 2, 'This': 3, 'movie': 4, 'is': 5, 'not': 6, 'scary': 7, 'and': 8, 'slow': 9}\n"
     ]
    }
   ],
   "source": [
    "#REVIEW 2\n",
    "\n",
    "def text2bow(words: List[str], dictionary: Dict[str, int]) -> List[Tuple[int, int]]:                  # Text to BOW\n",
    "    word_frequences = collections.defaultdict(int)\n",
    "    for word in words:\n",
    "        if word not in dictionary:                                                                    # Check condition\n",
    "            dictionary[word] = len(dictionary)\n",
    "        word_frequences[dictionary[word]] += 1\n",
    "    return list(word_frequences.items())                                                              # Return word frequencies\n",
    "sample_text = 'Review 2 : This movie is not scary and is slow'                                                    # Input text\n",
    "dictionary = {}                                                                                       # Initialize dictionary\n",
    "print('\\nBOW Representation: \\n', text2bow(sample_text.split(), dictionary))                          # print BOW\n",
    "print('Input Text:\\n',sample_text)                                                                    # print input\n",
    "print('\\nDictionary: \\n', dictionary)                                                                 # print dictionary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce6240c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOW Representation: \n",
      " [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n",
      "Input Text:\n",
      " Review 3 : This movie is spooky and good\n",
      "\n",
      "Dictionary: \n",
      " {'Review': 0, '3': 1, ':': 2, 'This': 3, 'movie': 4, 'is': 5, 'spooky': 6, 'and': 7, 'good': 8}\n"
     ]
    }
   ],
   "source": [
    "#REVIEW 3\n",
    "\n",
    "def text2bow(words: List[str], dictionary: Dict[str, int]) -> List[Tuple[int, int]]:                  # Text to BOW\n",
    "    word_frequences = collections.defaultdict(int)\n",
    "    for word in words:\n",
    "        if word not in dictionary:                                                                    # Check condition\n",
    "            dictionary[word] = len(dictionary)\n",
    "        word_frequences[dictionary[word]] += 1\n",
    "    return list(word_frequences.items())                                                              # Return word frequencies\n",
    "sample_text = 'Review 3 : This movie is spooky and good'                                                          # Input text\n",
    "dictionary = {}                                                                                       # Initialize dictionary\n",
    "print('\\nBOW Representation: \\n', text2bow(sample_text.split(), dictionary))                          # print BOW\n",
    "print('Input Text:\\n',sample_text)                                                                    # print input\n",
    "print('\\nDictionary: \\n', dictionary)                                                                 # print dictionary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2af56cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Length:\n",
      " (96,)\n",
      "Word Vector Representation:\n",
      " [ 0.08444607  0.12996519  0.25090355 -0.04837775  1.1570647   1.0232297\n",
      "  0.51766723  0.63749    -0.21312581 -0.2526669  -0.72337085 -0.11484593\n",
      "  2.056591   -0.43503147  0.0512511  -0.7099442  -0.35425502 -0.8625312\n",
      "  0.9220847   0.08002502 -0.22490364 -0.0218206  -0.45728493 -0.85805416\n",
      "  0.17037234 -0.01779819 -0.18357205 -1.2258139  -0.32962835 -0.8643147\n",
      " -0.42144     0.5384248   0.04062448  0.50345683 -0.17407241 -2.0512395\n",
      " -0.48872298  1.2492242   0.53873146  1.6154748  -0.6652746  -0.76472753\n",
      "  0.5348292   0.09983801 -1.4636991  -0.14339338 -0.15525445  0.29389852\n",
      "  1.1805356  -0.11594096  0.19914103 -0.20053369 -0.03914784 -0.51784486\n",
      " -0.06851092 -0.40133905 -0.3348826  -0.21894021 -0.59389377  0.11161892\n",
      "  0.02002376  2.1144261   1.060673   -1.3011045   0.5552813  -1.0625291\n",
      " -0.6888257  -0.40453744  0.22568427 -0.35341474 -0.9439845   0.19931321\n",
      " -1.2395508  -0.34215322  0.66172147 -0.6072136   0.27366182  1.2309372\n",
      "  0.23297769 -1.5352497   0.9554155  -0.2275597  -0.751639   -0.43210596\n",
      "  1.9347973  -0.20531842  0.49412692  0.08929562 -0.29858422  0.4656142\n",
      " -1.2512507  -0.61888266 -0.66638255  0.8072786   1.3016292  -0.0033237 ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Review')                                       # for single word\n",
    "print('Vector Length:\\n',doc.vector.shape)                # length of vector\n",
    "print('Word Vector Representation:\\n',doc.vector)         # print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87f45ec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Length:\n",
      " (96,)\n",
      "Word Vector Representation:\n",
      " [-1.230675    1.3622421   0.41508293  0.23442823 -0.39748314 -0.49457446\n",
      " -0.13032854  0.75998557 -0.21790811 -0.05159    -0.6336853  -0.17683047\n",
      " -0.03390747  1.0648445   0.45245054 -1.7207558   0.64101887 -0.70688057\n",
      "  1.443763    0.18873107  0.20001148 -0.6011127  -0.9655781   0.00813657\n",
      " -0.6132463  -1.4529294  -0.1304617   1.1534377  -0.5185296  -0.23840518\n",
      "  0.41837204 -1.3561993   0.13022666  1.0625572  -0.16988535 -0.8372294\n",
      "  0.2245235   0.81401145  0.29627255  1.2650975  -0.93742204 -0.9593301\n",
      "  0.531508   -0.04786337 -0.28503162 -0.44319257 -0.14379951 -0.1968005\n",
      "  1.2327617   1.5501475   0.048411    0.02398767 -1.095253    0.9863709\n",
      "  0.9576199   0.27519786 -0.2514766  -0.7966854   0.0528297   0.388033\n",
      " -0.791499   -0.21451102  0.2082287  -0.02687192 -0.19569847 -0.73481524\n",
      " -0.2052333  -0.30613923 -0.03286523 -0.5631656   0.1688798  -0.4884945\n",
      " -0.06226408  0.0807623   1.3848456   0.14010814 -0.26180255  1.5808759\n",
      "  0.08571005 -1.7201178   0.73583585 -0.14736518 -0.5925213   1.3376604\n",
      " -0.21971792 -0.42009407 -1.1805956  -0.796094   -0.96569943  0.76938903\n",
      "  2.3901756  -0.24846211  0.8554774  -0.18402372  0.89721775 -0.04872503]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'scary')                                       # for single word\n",
    "print('Vector Length:\\n',doc.vector.shape)                # length of vector\n",
    "print('Word Vector Representation:\\n',doc.vector)         # print output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9dc600",
   "metadata": {},
   "source": [
    "### TASK - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01e6881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey siri\n"
     ]
    }
   ],
   "source": [
    "# Pattern 1\n",
    "\n",
    "from spacy.matcher import Matcher                                             # Import matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")                                            # Load model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"hey\"}, {\"LOWER\": \"siri\"}]                            # Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "matcher.add(\"HeySiri\", [pattern])\n",
    "doc = nlp(\"Hey, Siri! Hey siri!\")                                       # Input text\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:                                          # Find matches\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    #print(match_id, string_id, start, end, span.text)\n",
    "    print(span.text)                                                          # print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f496ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, Siri\n"
     ]
    }
   ],
   "source": [
    "# Pattern 2\n",
    "\n",
    "from spacy.matcher import Matcher                                             # Import matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")                                            # Load model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"hey\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"siri\"}]        # Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "#pattern = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"Heysiri\", [pattern])\n",
    "doc = nlp(\"Hey, Siri! Hey Siri!\")                                       # Input text\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:                                          # Find matches\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    #print(match_id, string_id, start, end, span.text)\n",
    "    print(span.text)                                                          # print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58627668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl (777.4 MB)\n",
      "     ----------                           221.3/777.4 MB 327.3 kB/s eta 0:28:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\http\\client.py\", line 462, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\http\\client.py\", line 506, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 167, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 205, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 339, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 75, in resolve\n",
      "    collected = self.factory.collect_root_requirements(root_reqs)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 506, in collect_root_requirements\n",
      "    req = self._make_requirement_from_install_req(\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 468, in _make_requirement_from_install_req\n",
      "    cand = self._make_candidate_from_link(\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 215, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 288, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 158, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 227, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 299, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 487, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 532, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 214, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 94, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 146, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 304, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl (777.4 MB)\n",
      "     ------------------------------------ 777.4/777.4 MB 480.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from en-core-web-lg==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (21.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (60.9.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\manish\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "Collecting en-core-web-md==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
      "     ---------------------------------------- 45.7/45.7 MB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from en-core-web-md==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (60.9.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\manish\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\manish\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "### Download the following Models \n",
    "!python -m spacy download en_core_web_lg\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132fed7a",
   "metadata": {},
   "source": [
    "# TASK - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa73424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text= apple , Vector= True , OOV= False\n",
      "Text= orange , Vector= True , OOV= False\n",
      "Text= pikkstn , Vector= False , OOV= True\n",
      "Text= German , Vector= True , OOV= False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")                                                    # Load model    \n",
    "doc = nlp(\"apple orange pikkstn German\")                                                # Input text\n",
    "for token in doc:\n",
    "    print('Text=',token.text,', Vector=',token.has_vector,', OOV=', token.is_oov)     # Check words in vocab or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f937cc",
   "metadata": {},
   "source": [
    "### TASK - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5fb1395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched based on lowercase token text: rotten mangoes\n",
      "Matched based on lowercase token text: sweet oranges\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in [\"ROTTEN mangoes\", \"sweet oranges\"]]\n",
    "matcher.add(\"FRUIT\", patterns)\n",
    "doc = nlp(\"Do not put rotten mangoes and sweet oranges together.\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on lowercase token text:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3320b",
   "metadata": {},
   "source": [
    "### TASK - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbec9414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Length:\n",
      " (300,)\n",
      "Word Vector Representation:\n",
      " [ 2.24614009e-01  6.87758625e-02 -1.73480585e-01  1.65805761e-02\n",
      "  1.34624004e-01  5.45591712e-02  2.19285907e-03 -3.95957045e-02\n",
      "  6.82792813e-02  1.97639692e+00 -4.61678565e-01 -2.92714275e-02\n",
      "  1.66712284e-01  3.05328630e-02 -1.20274566e-01 -7.90856779e-03\n",
      " -2.90945709e-01  1.11722434e+00  6.90842792e-02 -3.84949967e-02\n",
      "  1.04422286e-01  1.60053283e-01 -8.62417221e-02 -1.08257428e-01\n",
      " -1.23409428e-01 -8.73568282e-02 -2.03284860e-01 -2.21845388e-01\n",
      "  1.08609453e-01  1.16122566e-01 -1.37289673e-01  3.65837142e-02\n",
      " -5.93178980e-02  2.00193852e-01 -5.70568405e-02  2.11971134e-01\n",
      "  3.52973565e-02  3.17453407e-02 -1.05713382e-01 -1.46389633e-01\n",
      "  1.32286716e-02 -7.17225596e-02 -8.99554342e-02  3.06991637e-02\n",
      "  1.77117258e-01  1.83081269e-01 -1.73389629e-01 -4.74941507e-02\n",
      "  1.44464582e-01  4.47510071e-02 -1.53409272e-01 -9.95019898e-02\n",
      " -1.11263432e-01  9.11899880e-02 -4.40797508e-02  3.18064317e-02\n",
      " -9.49835777e-02 -7.99477175e-02  6.82714209e-02 -2.72716433e-01\n",
      " -7.04699755e-02 -2.15631843e-01  3.83200380e-03  1.23324849e-01\n",
      " -1.05222002e-01  2.18019970e-02  7.53409937e-02  6.64012879e-02\n",
      "  5.76176718e-02  1.13126434e-01  4.69998531e-02  1.11280851e-01\n",
      "  1.67995811e-01 -2.42734291e-02  2.68738419e-01  3.02351508e-02\n",
      " -1.19733460e-01  2.66472548e-01  2.20624320e-02  2.70356447e-01\n",
      "  1.76692575e-01  1.13471434e-01 -9.14802924e-02  4.27372120e-02\n",
      "  2.24691585e-01 -1.80135891e-01  1.79722443e-01 -3.48288640e-02\n",
      "  2.45031863e-01  1.14812434e-01 -6.75501376e-02  3.78184319e-02\n",
      " -4.06094268e-02 -1.97365850e-01  1.23469710e-01  6.51817173e-02\n",
      "  4.40741405e-02  5.92652485e-02 -1.29514318e-02  9.26917121e-02\n",
      "  1.45930007e-01 -1.34802848e-01 -9.80311409e-02  8.24061483e-02\n",
      " -2.19577160e-02 -8.73254597e-01  1.37932286e-01 -2.55044270e-02\n",
      "  3.13982852e-02 -2.85028554e-02  7.50634298e-02 -3.52958262e-01\n",
      "  1.58934712e-01  1.21005755e-02  5.03927581e-02 -9.99782886e-03\n",
      "  6.19997121e-02  1.42192572e-01 -1.46715149e-01  1.34811431e-01\n",
      " -2.21408568e-02  2.78362893e-02  4.85582873e-02 -1.83265999e-01\n",
      " -1.10999988e-02  1.50330707e-01  1.07682995e-01  1.60008520e-01\n",
      "  1.28131425e-02 -7.98697099e-02 -9.57856886e-03 -1.87373996e-01\n",
      " -2.06519410e-01  4.92850021e-02  1.75792143e-01 -1.52349859e-01\n",
      " -6.19215854e-02  1.30162582e-01 -5.42093851e-02 -9.71291438e-02\n",
      " -1.43836713e+00 -2.41807103e-02  1.09650707e-02 -1.55117422e-01\n",
      "  1.22585678e-02  1.19962566e-01 -2.49488443e-01  8.95104334e-02\n",
      "  2.60345694e-02 -1.96896523e-01 -2.50399292e-01 -2.41710003e-02\n",
      " -7.43167177e-02 -7.58137181e-02  9.77285393e-03  1.53544294e-02\n",
      " -2.05942779e-03  7.40898475e-02  1.36092857e-01 -3.70638557e-02\n",
      " -7.32331425e-02 -3.60642583e-03 -9.98875126e-02  4.79622856e-02\n",
      "  8.86249989e-02 -1.78650573e-01 -6.36818558e-02 -4.69678566e-02\n",
      "  1.60917137e-02  1.21777721e-01 -1.59746140e-01 -3.60942748e-03\n",
      "  1.76757183e-02 -2.32492998e-01 -1.82902832e-02  4.32114536e-03\n",
      " -1.05152145e-01 -9.25247073e-02 -3.91714601e-03 -1.06186174e-01\n",
      " -3.79522853e-02 -2.39761442e-01 -1.68932870e-01 -1.14826728e-02\n",
      "  2.05940709e-01  1.31051704e-01 -6.33980557e-02 -7.37955645e-02\n",
      "  3.51048484e-02 -9.72729083e-03  6.63594306e-02  6.30172864e-02\n",
      " -8.01628828e-03 -2.54577130e-01 -1.43386260e-01  2.44084433e-01\n",
      "  5.08924313e-02 -6.78215697e-02 -6.20557256e-02  1.27285928e-03\n",
      " -2.47729896e-03 -3.21350060e-02  2.48581413e-02  5.26439957e-02\n",
      "  2.54101425e-01  1.09174307e-02  5.84604256e-02 -6.59365281e-02\n",
      "  6.96245655e-02  1.19115710e-01  8.08625743e-02  7.62149692e-04\n",
      " -2.56992877e-02 -1.03403144e-01 -1.13146856e-01  1.34597287e-01\n",
      "  3.59622836e-02 -1.89962283e-01 -9.89040136e-02  2.87565291e-01\n",
      "  2.47294143e-01 -3.21268827e-01  2.20075715e-02 -3.62617113e-02\n",
      " -1.00194283e-01 -5.35355769e-02  8.38532895e-02  2.02406138e-01\n",
      " -2.22657137e-02  1.92345992e-01  1.21704563e-01  2.31592134e-01\n",
      " -2.51478571e-02  1.09177768e-01  2.97285765e-02 -1.15107156e-01\n",
      " -9.63482782e-02 -2.13996276e-01 -1.80871487e-02  2.67722011e-01\n",
      "  2.89725699e-02  1.25016287e-01  6.59812838e-02  1.01202810e-02\n",
      "  1.28356293e-01 -1.34035856e-01 -2.78268550e-02  6.75581470e-02\n",
      " -8.42535943e-02  6.12615757e-02  1.11973152e-01 -2.00212449e-01\n",
      "  3.90320010e-02 -6.30311444e-02  2.14430705e-01  1.71186134e-01\n",
      "  1.24553718e-01 -1.63447991e-01 -1.13984145e-01 -1.46086141e-01\n",
      "  3.23140085e-01 -6.52028546e-02  9.01521891e-02 -7.47292712e-02\n",
      "  5.78784272e-02 -2.89538711e-01 -3.50512899e-02  2.25691423e-01\n",
      " -6.40668646e-02  7.80236796e-02 -6.28815517e-02 -4.75400221e-03\n",
      " -2.02514064e-02  6.49108067e-02  5.40397167e-02  2.90914714e-01\n",
      "  1.34912897e-02 -1.49114281e-01  4.46088836e-02  2.21929878e-01\n",
      "  4.77260016e-02 -2.16865703e-01 -5.64742945e-02 -1.55101465e-02\n",
      "  1.14785433e-01 -2.96119954e-02  2.32770845e-01 -2.90847160e-02\n",
      "  8.24381262e-02  1.16836302e-01 -2.96475682e-02  2.38042563e-01\n",
      "  1.07555859e-01 -1.72734275e-01  9.55624208e-02  7.84028601e-03\n",
      "  1.29934281e-01  1.53303295e-01 -7.92286638e-03  2.46932283e-01]\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u'I prefer the morning flight through Denmark')         # for single word\n",
    "print('Vector Length:\\n',doc1.vector.shape)                        # length of vector\n",
    "print('Word Vector Representation:\\n',doc1.vector)                 # print output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8086f",
   "metadata": {},
   "source": [
    "Total vector length: 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061f6c8",
   "metadata": {},
   "source": [
    "### TASK - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e806ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text= Do , Vector= True , OOV= False\n",
      "Text= not , Vector= True , OOV= False\n",
      "Text= put , Vector= True , OOV= False\n",
      "Text= rotten , Vector= True , OOV= False\n",
      "Text= mangoes , Vector= True , OOV= False\n",
      "Text= and , Vector= True , OOV= False\n",
      "Text= sweet , Vector= True , OOV= False\n",
      "Text= oranges , Vector= True , OOV= False\n",
      "Text= together , Vector= True , OOV= False\n",
      "Text= . , Vector= True , OOV= False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")                                                    # Load model    \n",
    "doc = nlp(\"Do not put rotten mangoes and sweet oranges together.\")                    # Input text\n",
    "for token in doc:\n",
    "    print('Text=',token.text,', Vector=',token.has_vector,', OOV=', token.is_oov)     # Check words in vocab or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d3ad1c",
   "metadata": {},
   "source": [
    "#### A) False, because the word \"rotten\" and \"sweet\" are not the out of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5c5d6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do Do 1.0\n",
      "Do not 0.7205888032913208\n",
      "Do put 0.6295328140258789\n",
      "Do rotten 0.23457235097885132\n",
      "Do mangoes 0.07004779577255249\n",
      "Do and 0.40728649497032166\n",
      "Do sweet 0.27546998858451843\n",
      "Do oranges 0.184742733836174\n",
      "Do together 0.4501313865184784\n",
      "Do . 0.3056337535381317\n",
      "not Do 0.7205888032913208\n",
      "not not 1.0\n",
      "not put 0.6083958148956299\n",
      "not rotten 0.2806089222431183\n",
      "not mangoes 0.10161228477954865\n",
      "not and 0.5304263830184937\n",
      "not sweet 0.3388059437274933\n",
      "not oranges 0.17835381627082825\n",
      "not together 0.448627233505249\n",
      "not . 0.4248487055301666\n",
      "put Do 0.6295328140258789\n",
      "put not 0.6083958148956299\n",
      "put put 1.0\n",
      "put rotten 0.3163799047470093\n",
      "put mangoes 0.12202072143554688\n",
      "put and 0.49793902039527893\n",
      "put sweet 0.3838925063610077\n",
      "put oranges 0.22029344737529755\n",
      "put together 0.6148674488067627\n",
      "put . 0.390465646982193\n",
      "rotten Do 0.23457235097885132\n",
      "rotten not 0.2806089222431183\n",
      "rotten put 0.3163799047470093\n",
      "rotten rotten 1.0\n",
      "rotten mangoes 0.3282413184642792\n",
      "rotten and 0.2027491182088852\n",
      "rotten sweet 0.35820987820625305\n",
      "rotten oranges 0.3876389265060425\n",
      "rotten together 0.2033027857542038\n",
      "rotten . 0.13760976493358612\n",
      "mangoes Do 0.07004779577255249\n",
      "mangoes not 0.10161228477954865\n",
      "mangoes put 0.12202072143554688\n",
      "mangoes rotten 0.3282413184642792\n",
      "mangoes mangoes 1.0\n",
      "mangoes and 0.11509136110544205\n",
      "mangoes sweet 0.3816959261894226\n",
      "mangoes oranges 0.7255765795707703\n",
      "mangoes together 0.13692606985569\n",
      "mangoes . 0.03732023388147354\n",
      "and Do 0.40728649497032166\n",
      "and not 0.5304263830184937\n",
      "and put 0.49793902039527893\n",
      "and rotten 0.2027491182088852\n",
      "and mangoes 0.11509136110544205\n",
      "and and 1.0\n",
      "and sweet 0.41887080669403076\n",
      "and oranges 0.19245944917201996\n",
      "and together 0.5996914505958557\n",
      "and . 0.43241673707962036\n",
      "sweet Do 0.27546998858451843\n",
      "sweet not 0.3388059437274933\n",
      "sweet put 0.3838925063610077\n",
      "sweet rotten 0.35820987820625305\n",
      "sweet mangoes 0.3816959261894226\n",
      "sweet and 0.41887080669403076\n",
      "sweet sweet 1.0\n",
      "sweet oranges 0.4652591049671173\n",
      "sweet together 0.3930017352104187\n",
      "sweet . 0.2901315689086914\n",
      "oranges Do 0.184742733836174\n",
      "oranges not 0.17835381627082825\n",
      "oranges put 0.22029344737529755\n",
      "oranges rotten 0.3876389265060425\n",
      "oranges mangoes 0.7255765795707703\n",
      "oranges and 0.19245944917201996\n",
      "oranges sweet 0.4652591049671173\n",
      "oranges oranges 1.0\n",
      "oranges together 0.22797814011573792\n",
      "oranges . 0.14070969820022583\n",
      "together Do 0.4501313865184784\n",
      "together not 0.448627233505249\n",
      "together put 0.6148674488067627\n",
      "together rotten 0.2033027857542038\n",
      "together mangoes 0.13692606985569\n",
      "together and 0.5996914505958557\n",
      "together sweet 0.3930017352104187\n",
      "together oranges 0.22797814011573792\n",
      "together together 1.0\n",
      "together . 0.3799852728843689\n",
      ". Do 0.3056337535381317\n",
      ". not 0.4248487055301666\n",
      ". put 0.390465646982193\n",
      ". rotten 0.13760976493358612\n",
      ". mangoes 0.03732023388147354\n",
      ". and 0.43241673707962036\n",
      ". sweet 0.2901315689086914\n",
      ". oranges 0.14070969820022583\n",
      ". together 0.3799852728843689\n",
      ". . 1.0\n"
     ]
    }
   ],
   "source": [
    "for token1 in doc:                                                      # For token1\n",
    "    for token2 in doc:                                                  # For token 2\n",
    "        print(token1.text, token2.text, token1.similarity(token2))      # check similarity of token 1 with token 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065a8f7",
   "metadata": {},
   "source": [
    "#### B) Similarity values belween: mangoes oranges 0.7255765795707703"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6df685",
   "metadata": {},
   "source": [
    "#### C) Similarity values between: sweet oranges 0.4652591049671173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f1138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
